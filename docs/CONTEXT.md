# Cortex Runtime - Project Context

> **Purpose of this document:** Provide full context for human collaborators and AI subagents working on this project. Read this first before making changes.

## The Big Picture

We're building a **social gaming/messaging platform** with these elements:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     THE PLATFORM VISION                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  ğŸ“± Messaging App (core)                                        â”‚
â”‚      + ğŸ® Virtual Worlds (office, campus, custom)               â”‚
â”‚      + ğŸ¤– AI NPCs that live, learn, remember                    â”‚
â”‚      + ğŸ­ ARG storylines (mysteries, events)                    â”‚
â”‚      + ğŸ‘¥ Multiplayer (real players + AI)                       â”‚
â”‚      + ğŸ“ Location-based social (Pokemon Go style)              â”‚
â”‚      + ğŸ“º Streaming integration (Twitch/YouTube)                â”‚
â”‚                                                                  â”‚
â”‚  Think: Discord + The Sims + Pokemon Go + ARG + Twitch          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Cortex Matters

Cortex is the **runtime that powers the AI NPCs**. Each NPC needs to:
- Have persistent memory (remember players, events, relationships)
- Maintain state across sessions (personality, skills, goals)
- Execute behaviors (daily routines, role-playing, reactions)
- Feel alive (not just a chatbot - they have their own lives)

## Related Projects

| Project | Location | Purpose |
|---------|----------|---------|
| **cortex** | `/Users/loopy/Developer/ai/cortex` | Native AI runtime - memory, state, inference |
| **neural_assembly** | `/Users/loopy/Developer/ai/neural_assembly` | Research: teaching transformers CPU-like execution |

### How They Connect

```
neural_assembly (research)          cortex (runtime)
â”œâ”€â”€ Differentiable PC          â†’    Agent behavior execution
â”œâ”€â”€ Addressable Memory         â†’    Memory subsystem
â”œâ”€â”€ Call Stack                 â†’    Context/role switching
â”œâ”€â”€ CPU-like instructions      â†’    High-level life actions
â””â”€â”€ Learned execution traces   â†’    NPC decision making
```

The neural_assembly concepts may eventually inform how cortex agents "think" - but for now, cortex focuses on practical runtime infrastructure.

## Current State of Cortex

### Architecture Overview

```
cortex/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs              # Core types, Message, Role enums
â”‚   â”œâ”€â”€ main.rs             # CLI application
â”‚   â”œâ”€â”€ runtime.rs          # Cortex struct - main runtime
â”‚   â”œâ”€â”€ session.rs          # Persistent session wrapper
â”‚   â”œâ”€â”€ config.rs           # Configuration structures
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ mod.rs          # Memory subsystem (store + retrieve)
â”‚   â”‚   â””â”€â”€ vector.rs       # Vector store with cosine similarity
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ mod.rs          # InferenceEngine trait, chat templates
â”‚   â”‚   â””â”€â”€ llama.rs        # llama.cpp backend implementation
â”‚   â””â”€â”€ state/
â”‚       â”œâ”€â”€ mod.rs          # StateManager for persistence
â”‚       â””â”€â”€ checkpoint.rs   # Checkpoint and Branch structs
â””â”€â”€ Cargo.toml
```

### What Works

| Component | Status | Notes |
|-----------|--------|-------|
| Chat/Generation | âœ… Working | Streaming responses via llama.cpp |
| Session Persistence | âœ… Working | Auto-saves to ~/.local/share/cortex/sessions/ |
| Message History | âœ… Working | Tracks conversation with roles |
| Memory Storage | âš ï¸ Partial | Structure works, but search is broken |
| Checkpoint API | âš ï¸ Partial | Can save/load metadata, not actual KV cache |
| CLI Interface | âœ… Working | chat, generate, sessions commands |
| Configuration | âœ… Working | Model path, GPU layers, context size, etc. |

### What's Broken

| Issue | Location | Impact | Priority |
|-------|----------|--------|----------|
| **Embeddings return zeros** | `src/inference/llama.rs:164` | Memory search always fails | ğŸ”´ Critical |
| **KV cache not serialized** | `src/inference/llama.rs:279` | Can't truly restore state | ğŸ”´ Critical |
| **No API server** | N/A | CLI only, can't serve platform | ğŸŸ¡ High |
| **Single agent only** | N/A | Can't run multiple NPCs | ğŸŸ¡ High |
| **No event system** | N/A | Can't react to world events | ğŸŸ  Medium |

### Code Locations for Key Issues

**Embedding stub (returns zeros):**
```rust
// src/inference/llama.rs:164
// TODO: Use proper sentence embedding model or pooling strategy
let _tokens = self.tokenize(text, false)?;
Ok(vec![0.0; self.embedding_dim()])  // <-- BROKEN: always zeros
```

**KV cache stub (not implemented):**
```rust
// src/inference/llama.rs:279
Ok(KvCacheState {
    data: vec![],  // TODO: Implement actual cache extraction
    n_tokens: self.tokens_in_context,
    model_id: self.model_id.clone(),
})
```

## Target Architecture (v1)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CORTEX v1 RUNTIME                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    API LAYER                             â”‚    â”‚
â”‚  â”‚  HTTP/WebSocket server for platform integration          â”‚    â”‚
â”‚  â”‚  - POST /agent/{id}/message  (chat with NPC)            â”‚    â”‚
â”‚  â”‚  - GET  /agent/{id}/state    (inspect NPC state)        â”‚    â”‚
â”‚  â”‚  - POST /agent/{id}/event    (world event notification) â”‚    â”‚
â”‚  â”‚  - WS   /agent/{id}/stream   (streaming responses)      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                  AGENT MANAGER                           â”‚    â”‚
â”‚  â”‚  Manages multiple agent instances                        â”‚    â”‚
â”‚  â”‚  - Agent pool with lifecycle management                  â”‚    â”‚
â”‚  â”‚  - Shared model inference (one model, many agents)       â”‚    â”‚
â”‚  â”‚  - Per-agent state isolation                             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   AGENT 1    â”‚ â”‚   AGENT 2    â”‚ â”‚   AGENT N    â”‚             â”‚
â”‚  â”‚   (Maya)     â”‚ â”‚   (Bob)      â”‚ â”‚   (...)      â”‚             â”‚
â”‚  â”‚              â”‚ â”‚              â”‚ â”‚              â”‚             â”‚
â”‚  â”‚ - Session    â”‚ â”‚ - Session    â”‚ â”‚ - Session    â”‚             â”‚
â”‚  â”‚ - Memory     â”‚ â”‚ - Memory     â”‚ â”‚ - Memory     â”‚             â”‚
â”‚  â”‚ - State      â”‚ â”‚ - State      â”‚ â”‚ - State      â”‚             â”‚
â”‚  â”‚ - Personalityâ”‚ â”‚ - Personalityâ”‚ â”‚ - Personalityâ”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                              â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                 INFERENCE ENGINE                         â”‚    â”‚
â”‚  â”‚  Shared llama.cpp instance                               â”‚    â”‚
â”‚  â”‚  - Model loaded once                                     â”‚    â”‚
â”‚  â”‚  - Request queue for fair scheduling                     â”‚    â”‚
â”‚  â”‚  - Working embeddings (sentence-transformers or pooling) â”‚    â”‚
â”‚  â”‚  - KV cache management per agent                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                 PERSISTENCE LAYER                        â”‚    â”‚
â”‚  â”‚  - Agent state (memory, checkpoints)                     â”‚    â”‚
â”‚  â”‚  - Session history                                       â”‚    â”‚
â”‚  â”‚  - Vector store (with real embeddings)                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Decisions Made

1. **Rust stays** - Performance matters for multi-agent runtime
2. **llama.cpp backend** - Local inference, no API dependencies
3. **No external DBs for v1** - File-based persistence (SQLite maybe later)
4. **HTTP + WebSocket API** - Standard integration with platform
5. **Shared model, isolated state** - Efficient multi-agent support

## Key Decisions Needed

- [ ] Embedding strategy: Use separate embedding model or pool from LLM?
- [ ] API framework: axum, actix-web, or warp?
- [ ] Agent scheduling: Round-robin, priority queue, or async all?
- [ ] State format: Keep bincode or switch to something debuggable?

## For AI Subagents

When working on this codebase:

1. **Read this file first** - Understand the vision and current state
2. **Check ROADMAP.md** - See what's currently being worked on
3. **Don't over-engineer** - We're targeting a weekend v1
4. **Keep it working** - Small, tested increments
5. **Update docs** - If you learn something, document it

### Common Tasks

**Adding a new API endpoint:**
1. Add route in `src/api/mod.rs` (once created)
2. Add handler in appropriate module
3. Update this doc if it changes the architecture

**Fixing the embedding issue:**
1. Look at `src/inference/llama.rs:164`
2. Options: mean pooling, separate model, or external service
3. Test with `src/memory/vector.rs` similarity search

**Adding a new agent capability:**
1. Consider if it belongs in Agent, Session, or Runtime
2. Keep state serializable (serde)
3. Think about multi-agent implications

## Contact / Collaboration

This is a hyper-collaborative project. When in doubt:
- Document your assumptions
- Ask clarifying questions
- Make small PRs/changes
- Keep the runtime working
